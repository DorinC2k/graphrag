{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import graphrag.api as api\n",
    "from graphrag.config.create_graphrag_config import create_graphrag_config\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.config.models.vector_store_config import VectorStoreConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Search example\n",
    "\n",
    "Global search method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole (e.g. What are the most significant values of the herbs mentioned in this notebook?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM setup\n",
    "\n",
    "Set the `GRAPHRAG_API_KEY`, `GRAPHRAG_LLM_MODEL`, and `GRAPHRAG_EMBEDDING_MODEL` environment variables before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = os.environ[\"GRAPHRAG_LLM_MODEL\"]\n",
    "embedding_model = os.environ[\"GRAPHRAG_EMBEDDING_MODEL\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and configure GraphRAG\n",
    "\n",
    "- Configure the chat and embedding models used for queries.\n",
    "- Load the community, entity, and report tables that back global search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet files generated from indexing pipeline\n",
    "INPUT_DIR = \"./inputs/operation dulce\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "\n",
    "# community level in the Leiden community hierarchy from which we will load the community reports\n",
    "# higher value means we use reports from more fine-grained communities (at the cost of higher computation cost)\n",
    "COMMUNITY_LEVEL = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_config = LanguageModelConfig(\n",
    "    api_key=api_key,\n",
    "    type=ModelType.OpenAIChat,\n",
    "    model=llm_model,\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "embedding_config = LanguageModelConfig(\n",
    "    api_key=api_key,\n",
    "    type=ModelType.OpenAIEmbedding,\n",
    "    model=embedding_model,\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "vector_store_config = VectorStoreConfig(\n",
    "    type=\"lancedb\",\n",
    "    db_uri=str(Path(INPUT_DIR).resolve() / \"lancedb\"),\n",
    "    container_name=\"default\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "config = create_graphrag_config(\n",
    "    {\n",
    "        \"models\": {\n",
    "            \"default_chat_model\": chat_config,\n",
    "            \"default_embedding_model\": embedding_config,\n",
    "        },\n",
    "        \"global_search\": {\n",
    "            \"chat_model_id\": \"default_chat_model\",\n",
    "        },\n",
    "        \"vector_store\": {\n",
    "            \"default_vector_store\": vector_store_config,\n",
    "        },\n",
    "    },\n",
    "    root_dir=Path(\".\"),\n",
    ")\n",
    "\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "\n",
    "print(f\"Total community report count: {len(report_df)}\")\n",
    "report_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run global search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, context = await api.global_search(\n",
    "    config=config,\n",
    "    entities=entity_df,\n",
    "    communities=community_df,\n",
    "    community_reports=report_df,\n",
    "    community_level=COMMUNITY_LEVEL,\n",
    "    dynamic_community_selection=False,\n",
    "    response_type=\"Multiple Paragraphs\",\n",
    "    query=\"What is operation dulce?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect response and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context[\"reports\"].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}